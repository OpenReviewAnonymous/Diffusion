#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Calculation and Visualization of Evaluation Metrics for Time Series Generation Models
"""

# =============== Import necessary libraries ===============
from networkx import difference
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew, entropy
from sklearn.neighbors import KernelDensity
import ot  # Requires installation: pip install pot
from sympy import Ordinal
from tqdm import tqdm
from datetime import datetime
from PIL import Image

import adaptive_ddpm

# New: Import iisignature
import iisignature as _iisig


timestamp = datetime.now().strftime("%Y-%m-%d_%H:%M")


# Import from diffwave
try:
    from diffwave import N
    from diffwave import load_data
    N_test = int(0.12 * N)
except ImportError:
    print("Warning: Could not import diffwave module")

# =============== Data Loading Functions ===============
def load_datasets():
    """Loads all experimental datasets"""
    # Real data
    real_data = np.load("test_samples_normalized_2025-03-28_05:11.npy")
    print("Real data shape:", real_data.shape)


    # Attempt to load data generated by DDPM model
    ddpm_data_path = "samples_ddpm_2025-04-23_17-43.npy"
    ddpm_data = np.load(ddpm_data_path)
    ddpm_model_name = ddpm_data_path.split("_")[1]  # Extract "ddpm"
    print(f"{ddpm_model_name} data shape:", ddpm_data.shape)


    # Data generated by DiffWave model
    # diffwave_data_path = "simulated_data_2025-03-30_16:13_size10000.npy"
    diffwave_data_path = "samples_diffwave_2025-03-28_04:59.npy"
    diffwave_data = np.load(diffwave_data_path)
    diffwave_data =  diffwave_data
    diffwave_model_name = diffwave_data_path.split("_")[1]  # Extract "diffwave"
    print(f"{diffwave_model_name} data shape:", diffwave_data.shape)

    # Data generated by Adaptive model
    adaptive_data_path = "samples_adaptive_ddpm_2025-03-28_09-02.npy"
    adaptive_diffwave_data = np.load(adaptive_data_path)
    # Extract model name from filename (adaptive_ddpm)
    model_name_parts = adaptive_data_path.split("_")
    adaptive_model_name = f"{model_name_parts[1]}_{model_name_parts[2]}"  # Extract "adaptive_ddpm"
    # adaptive_diffwave_data = np.load("samples_adaptive_2025-03-28_04-58.npy")
    print(f"{adaptive_model_name} data shape:", adaptive_diffwave_data.shape)

    # Load simulated data from file calibrated
    calibrated_data_path = "simulation_results/simulated_normalized_2025-05-11_06:17.npy"
    calibrated_data = np.load(calibrated_data_path)
    print(f"calibrated_data data shape:", calibrated_data.shape)

    return real_data, diffwave_data, adaptive_diffwave_data, diffwave_model_name, adaptive_model_name, calibrated_data, ddpm_data, ddpm_model_name

# =============== Data Statistics and Visualization Functions ===============
def plot_correlation_matrix(data, title="Correlation Matrix", save_path=None, vmin=-1, vmax=1):
    """
    Plots the correlation matrix heatmap for 2D time series data

    Parameters:
    data: Data with shape (N, T, 2), where N is the number of samples, T is the time steps, and 2 is the dimension
    title: Chart title
    save_path: Chart save path, uses title as filename if None
    vmin, vmax: Color range for the heatmap
    """
    # Extract X and Y sequences from (N, T, 2)
    N, T, dim = data.shape
    X_flat = data[:, :, 0]  # Extract data for the first dimension, shape (N, T)
    Y_flat = data[:, :, 1]  # Extract data for the second dimension, shape (N, T)

    # Concatenate X and Y into shape (N, 2T)
    flatten_data = np.hstack([X_flat, Y_flat])

    # Calculate the correlation matrix, rowvar=False means each column is a variable
    corr_matrix = np.corrcoef(flatten_data, rowvar=False)

    # Plot the correlation matrix heatmap
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        corr_matrix,
        cmap="RdBu_r",  # Red-blue diverging colormap
        center=0,       # Center the colormap at 0
        vmax=vmax, vmin=vmin,  # Use the provided uniform range
        square=True,
        fmt='.2f',
        cbar=True
    )

    # Draw separation lines at position T to divide the matrix into four blocks
    plt.axvline(x=T, color='k', linestyle='--')  # Vertical line
    plt.axhline(y=T, color='k', linestyle='--')  # Horizontal line

    # Add block labels
    plt.text(T/2, -5, "X-X", ha='center')
    plt.text(T+T/2, -5, "X-Y", ha='center')
    plt.text(-5, T/2, "Y-X", va='center', rotation=90)
    plt.text(-5, T+T/2, "Y-Y", va='center', rotation=90)

    plt.title(title)
    plt.tight_layout()

    # Save the chart
    if save_path is None:
        save_path = f"{title.replace(' ', '_')}.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Correlation matrix plot saved to: {save_path}")
    return corr_matrix

def plot_covariance_matrix(data, title="Covariance Matrix", save_path=None, vmin=None, vmax=None):
    """
    Plots the covariance matrix heatmap for 2D time series data

    Parameters:
    data: Data with shape (N, T, 2), where N is the number of samples, T is the time steps, and 2 is the dimension
    title: Chart title
    save_path: Chart save path, uses title as filename if None
    vmin, vmax: Color range for the heatmap
    """
    # Extract X and Y sequences from (N, T, 2)
    N, T, dim = data.shape
    X_flat = data[:, :, 0]  # Extract data for the first dimension, shape (N, T)
    Y_flat = data[:, :, 1]  # Extract data for the second dimension, shape (N, T)

    # Concatenate X and Y into shape (N, 2T)
    flatten_data = np.hstack([X_flat, Y_flat])

    # Calculate the covariance matrix
    cov_matrix = np.cov(flatten_data, rowvar=False)

    # Plot the covariance matrix heatmap
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        cov_matrix,
        cmap="RdBu_r",  # Red-blue diverging colormap
        center=0,       # Center the colormap at 0
        vmin=vmin, vmax=vmax,  # Use the provided uniform range
        square=True,
        fmt='.2f',
        cbar=True
    )

    # Draw separation lines at position T to divide the matrix into four blocks
    plt.axvline(x=T, color='k', linestyle='--')  # Vertical line
    plt.axhline(y=T, color='k', linestyle='--')  # Horizontal line

    # Add block labels
    plt.text(T/2, -5, "X-X", ha='center')
    plt.text(T+T/2, -5, "X-Y", ha='center')
    plt.text(-5, T/2, "Y-X", va='center', rotation=90)
    plt.text(-5, T+T/2, "Y-Y", va='center', rotation=90)

    plt.title(title)
    plt.tight_layout()

    # Save the chart
    if save_path is None:
        save_path = f"{title.replace(' ', '_')}.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Covariance matrix plot saved to: {save_path}")
    return cov_matrix

def plot_statistical_distributions(data, title="Statistical Distributions", save_path=None, xlims=None, ylims=None):
    """
    Plots the statistical distributions of 2D time series data

    Parameters:
    data: Data with shape (N, T, 2), where N is the number of samples, T is the time steps, and 2 is the dimension
    title: Chart title
    save_path: Chart save path, uses title as filename if None
    xlims: List of x-axis ranges for each subplot, format [(xmin1, xmax1), (xmin2, xmax2), ...]
    ylims: List of y-axis ranges for each subplot, format [(ymin1, ymax1), (ymin2, ymax2), ...]
    """
    N, T, dim = data.shape

    # Calculate statistics
    means_x = np.mean(data[:, :, 0], axis=1)  # Mean of X dimension
    means_y = np.mean(data[:, :, 1], axis=1)  # Mean of Y dimension

    vars_x = np.var(data[:, :, 0], axis=1)    # Variance of X dimension
    vars_y = np.var(data[:, :, 1], axis=1)    # Variance of Y dimension

    skew_x = skew(data[:, :, 0], axis=1)      # Skewness of X dimension
    skew_y = skew(data[:, :, 1], axis=1)      # Skewness of Y dimension

    # Create a 3x2 chart layout (3 statistics x 2 dimensions)
    fig, axes = plt.subplots(3, 2, figsize=(15, 12))

    # Mean distribution
    axes[0, 0].hist(means_x, bins=50, color='blue', alpha=0.7)
    axes[0, 0].set_title('X Dimension Mean Distribution')
    axes[0, 0].set_xlabel('Mean')
    axes[0, 0].set_ylabel('Frequency')

    axes[0, 1].hist(means_y, bins=50, color='red', alpha=0.7)
    axes[0, 1].set_title('Y Dimension Mean Distribution')
    axes[0, 1].set_xlabel('Mean')
    axes[0, 1].set_ylabel('Frequency')

    # Variance distribution
    axes[1, 0].hist(vars_x, bins=50, color='blue', alpha=0.7)
    axes[1, 0].set_xlim(0, 10)
    axes[1, 0].set_title('X Dimension Variance Distribution')
    axes[1, 0].set_xlabel('Variance')
    axes[1, 0].set_ylabel('Frequency')

    axes[1, 1].hist(vars_y, bins=50, color='red', alpha=0.7)
    axes[1, 1].set_xlim(0, 10)
    axes[1, 1].set_title('Y Dimension Variance Distribution')
    axes[1, 1].set_xlabel('Variance')
    axes[1, 1].set_ylabel('Frequency')

    # Skewness distribution
    axes[2, 0].hist(skew_x, bins=50, color='blue', alpha=0.7)
    axes[2, 0].set_title('X Dimension Skewness Distribution')
    axes[2, 0].set_xlabel('Skewness')
    axes[2, 0].set_ylabel('Frequency')

    axes[2, 1].hist(skew_y, bins=50, color='red', alpha=0.7)
    axes[2, 1].set_title('Y Dimension Skewness Distribution')
    axes[2, 1].set_xlabel('Skewness')
    axes[2, 1].set_ylabel('Frequency')

    # Set uniform axis ranges
    if xlims is not None:
        for i in range(3):
            for j in range(2):
                idx = i * 2 + j
                if idx < len(xlims) and xlims[idx] is not None:
                    axes[i, j].set_xlim(xlims[idx])


    axes[1, 0].set_xlim(0, 10)
    axes[1, 1].set_xlim(0, 10)


    if ylims is not None:
        for i in range(3):
            for j in range(2):
                idx = i * 2 + j
                if idx < len(ylims) and ylims[idx] is not None:
                    axes[i, j].set_ylim(ylims[idx])

    plt.suptitle(title, fontsize=16)
    plt.tight_layout()
    plt.subplots_adjust(top=0.92)  # Make room for the main title

    # Save the chart
    if save_path is None:
        save_path = f"{title.replace(' ', '_')}.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Statistical distribution plot saved to: {save_path}")
    return (means_x, means_y), (vars_x, vars_y), (skew_x, skew_y)

def data_summary(data, name="Dataset", save_path=None, save_dir="evaluation_figures", xlims=None, ylims=None):
    """Prints basic statistical information for the dataset, considering 2D data, and generates visualization charts"""
    print(f"\n====== {name} Statistics ======")
    print(f"Shape: {data.shape}")

    # X Dimension Statistics
    x_data = data[:, :, 0].flatten()
    print(f"X Dimension:")
    print(f"  Min: {x_data.min():.4f}")
    print(f"  1% Quantile: {np.quantile(x_data, 0.01):.4f}")
    print(f"  25% Quantile: {np.quantile(x_data, 0.25):.4f}")
    print(f"  75% Quantile: {np.quantile(x_data, 0.75):.4f}")
    print(f"  99% Quantile: {np.quantile(x_data, 0.99):.4f}")
    print(f"  Max: {x_data.max():.4f}")

    # Y Dimension Statistics
    y_data = data[:, :, 1].flatten()
    print(f"Y Dimension:")
    print(f"  Min: {y_data.min():.4f}")
    print(f"  1% Quantile: {np.quantile(y_data, 0.01):.4f}")
    print(f"  25% Quantile: {np.quantile(y_data, 0.25):.4f}")
    print(f"  75% Quantile: {np.quantile(y_data, 0.75):.4f}")
    print(f"  99% Quantile: {np.quantile(y_data, 0.99):.4f}")
    print(f"  Max: {y_data.max():.4f}")

    # Create visualization charts
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # Histogram
    axes[0, 0].hist(x_data, bins=50, color='blue', alpha=0.7)
    axes[0, 0].set_title('X Dimension Distribution Histogram')
    axes[0, 0].set_xlabel('Value')
    axes[0, 0].set_ylabel('Frequency')

    axes[0, 1].hist(y_data, bins=50, color='red', alpha=0.7)
    axes[0, 1].set_title('Y Dimension Distribution Histogram')
    axes[0, 1].set_xlabel('Value')
    axes[0, 1].set_ylabel('Frequency')

    # Box plot
    axes[1, 0].boxplot(x_data, vert=False, widths=0.7)
    axes[1, 0].set_title('X Dimension Box Plot')
    axes[1, 0].set_xlabel('Value')

    axes[1, 1].boxplot(y_data, vert=False, widths=0.7)
    axes[1, 1].set_title('Y Dimension Box Plot')
    axes[1, 1].set_xlabel('Value')

    # Set uniform axis ranges
    if xlims is not None:
        for i in range(2):
            for j in range(2):
                idx = i * 2 + j
                if idx < len(xlims) and xlims[idx] is not None:
                    axes[i, j].set_xlim(xlims[idx])

    if ylims is not None:
        for i in range(2):
            for j in range(2):
                idx = i * 2 + j
                if idx < len(ylims) and ylims[idx] is not None:
                    axes[i, j].set_ylim(ylims[idx])

    plt.suptitle(f"{name} Statistical Distribution", fontsize=16)
    plt.tight_layout()
    plt.subplots_adjust(top=0.9)  # Make room for the title

    # Save the chart - ensure no spaces in filename
    file_name = name.replace(' ', '_')
    if save_path is None:
        save_path = f"{save_dir}/{file_name}_summary.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"Data statistics plot saved to: {save_path}")

    # # Merge statistical plots of three datasets
    # if "adaptive" in name.lower():  # When the last dataset is processed
    #     summary_paths = [
    #         f"{save_dir}/Real_Data_summary.png",
    #         f"{save_dir}/Diffwave_Data_summary.png",
    #         f"{save_dir}/{file_name}_summary.png"
    #     ]
    #     merge_images(summary_paths, f"{save_dir}/merged_data_summaries.png")
    #     print(f"Statistical plots of three datasets merged and saved to: {save_dir}/merged_data_summaries.png")

    return save_path

# Image merging function
def merge_images(image_paths, output_path, border_size=5, border_color=(255, 0, 0)):
    """
    Horizontally stitches multiple images and adds a border

    Parameters:
    image_paths: List of image paths
    output_path: Output image path
    border_size: Border width
    border_color: Border color, RGB tuple
    """
    # Open all images
    images = [Image.open(p) for p in image_paths]

    # Get the size of the first image
    width, height = images[0].size

    # Set layout: 1 row, n columns
    rows = 1
    cols = len(images)

    # Add border to images
    def add_border(image, border_size=5, color=(255, 0, 0)):
        """Adds a border of specified color and width to a single image"""
        w, h = image.size
        # Create a new image slightly larger than the original to accommodate the border
        bordered_img = Image.new("RGB", (w + 2*border_size, h + 2*border_size), color)
        # Paste the original image into the center of the new image
        bordered_img.paste(image, (border_size, border_size))
        return bordered_img

    # Add borders
    images_with_border = [add_border(img, border_size=border_size, color=border_color) for img in images]

    # Get the size of images after adding borders
    bordered_width, bordered_height = images_with_border[0].size

    # Create the canvas for the final stitched image
    combined_width = cols * bordered_width
    combined_height = rows * bordered_height
    merged_image = Image.new('RGB', (combined_width, combined_height))

    # Paste each sub-image into the large image in order
    for idx, img in enumerate(images_with_border):
        # Calculate which column the current image should be placed in
        col = idx % cols

        # Calculate the top-left pixel coordinates for pasting
        x_offset = col * bordered_width
        y_offset = 0  # Only one row, so y offset is 0

        # Paste the image at the specified position
        merged_image.paste(img, (x_offset, y_offset))

    # Save the final stitched large image
    merged_image.save(output_path)
    print(f"Stitching complete, saved as {output_path}")
    return output_path

# =============== Evaluation Metric Calculation Functions ===============

# New: Helper function _to_numpy (from double_check_calibrated_garch.py)
def _to_numpy(arr) -> np.ndarray:
    """Ensure *arr* is a NumPy array on CPU."""
    if "torch" in str(type(arr)):
        try:
            import torch
            if isinstance(arr, torch.Tensor):
                return arr.detach().cpu().numpy()
        except ImportError:
            pass
    if not isinstance(arr, np.ndarray):
        raise TypeError(f"Input must be a numpy array or torch tensor, got {type(arr)}")
    return arr

# New: Helper function compute_average_signature (from double_check_calibrated_garch.py)
def compute_average_signature(
    paths, level: int
) -> np.ndarray | None:
    """Average truncated signature of *paths* (N, T, d)."""
    if _iisig is None:
        return None

    x = _to_numpy(paths)
    if x.ndim != 3:
        raise ValueError(f"paths must have shape (N, T, d), but got {x.shape}")
    n_paths, _, dim_paths = x.shape

    try:
        sig_len = _iisig.siglength(int(dim_paths), int(level))
        all_signatures = _iisig.sig(x, level)

        if all_signatures.ndim == 1 and n_paths == 1:
            all_signatures = all_signatures.reshape(1, sig_len)
        elif all_signatures.shape[0] != n_paths or all_signatures.shape[1] != sig_len:
            all_signatures = all_signatures.reshape(n_paths, sig_len)

        return all_signatures.mean(axis=0)
    except Exception as e:
        print(f"Error computing signature: {e}")
        return None

# New: Path Signature Distance Function
def path_signature_distance(
    paths_a,
    paths_b,
    level: int = 3,
) -> float | None:
    """
    Calculates the average Signature distance (Euclidean distance) between two sets of paths.
    (3) to measure distance between the stochastic processes we generate,
        we might want to compute the signature (up to a given level;
        for which there are packages that do it) and then compute the average
        across elements and compute the Euclidean distance between these vectors.
    """
    if _iisig is None:
        print("iisignature module not loaded, cannot compute signature distance.")
        return np.nan # Or None, depending on subsequent handling

    if paths_a.shape[2] != paths_b.shape[2]:
        raise ValueError(
            f"Path dimensions do not match: paths_a dimension {paths_a.shape[2]}, "
            f"paths_b dimension {paths_b.shape[2]}"
        )

    mean_sig_a = compute_average_signature(paths_a, level)
    mean_sig_b = compute_average_signature(paths_b, level)

    if mean_sig_a is None or mean_sig_b is None:
        return np.nan # Or None

    # Calculate Euclidean distance
    distance = np.linalg.norm(mean_sig_a - mean_sig_b)
    return float(distance)

def correlation_matrix_difference(data1, data2):
    """
    Calculates the Frobenius norm of the difference between the correlation matrices of two 2D datasets

    Parameters:
    data1, data2: Datasets with shapes (N, T, 2) and (M, T, 2)

    Returns:
    float: Frobenius norm of the correlation matrix difference
    """
    # Process data1
    N1, T, _ = data1.shape
    X1_flat = data1[:, :, 0]
    Y1_flat = data1[:, :, 1]
    flatten_data1 = np.hstack([X1_flat, Y1_flat])
    corr1 = np.corrcoef(flatten_data1, rowvar=False)

    # Process data2
    N2, T, _ = data2.shape
    X2_flat = data2[:, :, 0]
    Y2_flat = data2[:, :, 1]
    flatten_data2 = np.hstack([X2_flat, Y2_flat])
    corr2 = np.corrcoef(flatten_data2, rowvar=False)

    # Calculate the Frobenius norm of the difference
    diff = corr1 - corr2
    return np.linalg.norm(diff, ord='fro')  # Frobenius norm

def marginal_wasserstein_distance(real_data, gen_data, normalize=False):
    """
    Calculates the Marginal Wasserstein Distance (MarginalWD)

    MarginalWD calculates the Wasserstein distance for each dimension separately and then averages them,
    treating each dimension as an independent marginal distribution, ignoring inter-dimensional correlations.

    Parameters:
    real_data: Real data with shape (N, T, D)
    gen_data: Generated data with shape (M, T, D)
    normalize: Whether to normalize by the standard deviation of the real data

    Returns:
    float: Marginal Wasserstein Distance
    """
    N, T, D = real_data.shape

    total_wd = 0
    # Calculate Wasserstein distance for each dimension separately
    for d in range(D):
        # Flatten the data for each dimension into a 1D array
        real_dim = real_data[:, :, d].flatten()
        gen_dim = gen_data[:, :, d].flatten()

        # If normalization is required
        if normalize:
            real_std = np.std(real_dim)
            if real_std > 0:  # Avoid division by zero
                real_dim = real_dim / real_std
                gen_dim = gen_dim / real_std

        # Use 1D Wasserstein distance calculation (based on sorting, complexity O(n log n))
        # This is much more efficient than the general EMD algorithm and suitable for 1D data
        wd = ot.wasserstein_1d(real_dim, gen_dim)
        total_wd += wd

    # Return the average Wasserstein distance across all dimensions
    return total_wd / D

def sliced_wasserstein_2d(X, Y, num_projections=5000, normalize=False, seed=0):
    """
    Calculates 2D Sliced Wasserstein distance via Monte Carlo random direction projections
    Complexity O(KÂ·(N log N + M log M)), K=num_projections

    Parameters:
      X, Y: np.ndarray, shape=(N,2)/(M,2)
      num_projections: Number of projection directions
      normalize: Whether to first divide X, Y by their standard deviation along each dimension
      seed: Random seed
    Returns:
      float: Average Sliced Wasserstein distance
    """
    if normalize:
        stds = np.std(X, axis=0)
        nz = stds > 0
        X[:, nz] /= stds[nz]
        Y[:, nz] /= stds[nz]

    rng = np.random.RandomState(seed)
    angles = rng.rand(num_projections) * 2 * np.pi
    dirs = np.stack([np.cos(angles), np.sin(angles)], axis=1)  # (K,2)

    PX = X.dot(dirs.T)  # (N, K)
    PY = Y.dot(dirs.T)  # (M, K)

    sw_total = 0.0
    for k in range(num_projections):
        a = np.sort(PX[:, k])
        b = np.sort(PY[:, k])
        # Interpolate to align lengths if unequal
        if a.size > b.size:
            b = np.interp(np.linspace(0, 1, a.size),
                          np.linspace(0, 1, b.size), b)
        elif a.size < b.size:
            a = np.interp(np.linspace(0, 1, b.size),
                          np.linspace(0, 1, a.size), a)
        sw_total += np.mean(np.abs(a - b))
    return sw_total / num_projections

def conditional_wasserstein_distance(real_data, gen_data, normalize=False, n_projections=5000, use_ray=False):
    """
    Calculates the Conditional Wasserstein Distance (ConditionalWD)
    New parameter:
      use_ray: bool, Whether to use Ray for parallelization (default False)
    Returns:
      float: Conditional Wasserstein Distance
    """
    import numpy as np
    import ot

    N, T, D = real_data.shape

    def _compute_one(t):
        real_t = real_data[:, t, :].copy()
        gen_t  = gen_data[:, t, :].copy()
        # Normalize
        if normalize:
            stds = np.std(real_t, axis=0)
            nz   = stds > 0
            real_t[:, nz] /= stds[nz]
            gen_t[:, nz]  /= stds[nz]
        # If it's exactly 2D, use the sliced method for fast approximation
        if D == 2:
            return sliced_wasserstein_2d(real_t, gen_t,
                                         num_projections=n_projections,
                                         normalize=False,
                                         seed=42)

        # Multidimensional EMD (original logic)
        w_r = np.ones(N) / N
        w_g = np.ones(gen_t.shape[0]) / gen_t.shape[0]
        C   = ot.dist(real_t, gen_t, metric='euclidean')
        emd2_val = ot.emd2(w_r, w_g, C)
        return float(np.sqrt(emd2_val))

    # Execute in parallel or serially (keep original use_ray branch)
    if use_ray:
        import ray
        ray.init(ignore_reinit_error=True, num_cpus=T)
        real_ref = ray.put(real_data)
        gen_ref  = ray.put(gen_data)

        @ray.remote(num_cpus=1)
        def _ray_task(rd, gd, norm, tt):
            return _compute_one(tt)

        tasks   = [_ray_task.remote(real_ref, gen_ref, normalize, t) for t in range(T)]
        results = ray.get(tasks)
        ray.shutdown()
        return float(sum(results) / T)
    else:
        # Serial execution (can also add tqdm)
        total = sum(_compute_one(t) for t in tqdm(range(T)))
        return float(total / T)

# Frobenius Norm Difference
def frobenius_corr_diff(real_data, gen_data):
    """Calculates the Frobenius norm of the correlation matrix difference"""
    # Process data1
    N1, T, _ = real_data.shape
    X1_flat = real_data[:, :, 0]
    Y1_flat = real_data[:, :, 1]
    flatten_data1 = np.hstack([X1_flat, Y1_flat])
    corr_real = np.corrcoef(flatten_data1, rowvar=False)

    # Process data2
    N2, T, _ = gen_data.shape
    X2_flat = gen_data[:, :, 0]
    Y2_flat = gen_data[:, :, 1]
    flatten_data2 = np.hstack([X2_flat, Y2_flat])
    corr_gen = np.corrcoef(flatten_data2, rowvar=False)

    diff = corr_real - corr_gen
    return np.linalg.norm(diff, ord='fro')

# Mean Squared Error
def mse_corr_diff(real_data, gen_data):
    """Calculates the element-wise Mean Squared Error (MSE) of the correlation matrices"""
    # Process data1
    N1, T, _ = real_data.shape
    X1_flat = real_data[:, :, 0]
    Y1_flat = real_data[:, :, 1]
    flatten_data1 = np.hstack([X1_flat, Y1_flat])
    corr_real = np.corrcoef(flatten_data1, rowvar=False)

    # Process data2
    N2, T, _ = gen_data.shape
    X2_flat = gen_data[:, :, 0]
    Y2_flat = gen_data[:, :, 1]
    flatten_data2 = np.hstack([X2_flat, Y2_flat])
    corr_gen = np.corrcoef(flatten_data2, rowvar=False)

    diff = corr_real - corr_gen
    return np.mean(diff**2)

# Mean Absolute Error
def mae_corr_diff(real_data, gen_data):
    """Calculates the element-wise Mean Absolute Error (MAE) of the correlation matrices"""
    # Process data1
    N1, T, _ = real_data.shape
    X1_flat = real_data[:, :, 0]
    Y1_flat = real_data[:, :, 1]
    flatten_data1 = np.hstack([X1_flat, Y1_flat])
    corr_real = np.corrcoef(flatten_data1, rowvar=False)

    # Process data2
    N2, T, _ = gen_data.shape
    X2_flat = gen_data[:, :, 0]
    Y2_flat = gen_data[:, :, 1]
    flatten_data2 = np.hstack([X2_flat, Y2_flat])
    corr_gen = np.corrcoef(flatten_data2, rowvar=False)

    diff = corr_real - corr_gen
    return np.mean(np.abs(diff))

# Correlation of Correlation Matrices
def corr_of_corrs(real_data, gen_data):
    """Calculates the Pearson correlation coefficient between the flattened correlation matrices"""
    # Process data1
    N1, T, _ = real_data.shape
    X1_flat = real_data[:, :, 0]
    Y1_flat = real_data[:, :, 1]
    flatten_data1 = np.hstack([X1_flat, Y1_flat])
    corr_real = np.corrcoef(flatten_data1, rowvar=False)

    # Process data2
    N2, T, _ = gen_data.shape
    X2_flat = gen_data[:, :, 0]
    Y2_flat = gen_data[:, :, 1]
    flatten_data2 = np.hstack([X2_flat, Y2_flat])
    corr_gen = np.corrcoef(flatten_data2, rowvar=False)

    # Flatten into vectors
    vec_real = corr_real.flatten()
    vec_gen = corr_gen.flatten()

    # Calculate Pearson correlation coefficient
    r = np.corrcoef(vec_real, vec_gen)[0, 1]
    return r

# KL Divergence (Kullback-Leibler divergence)
def kl_divergence(real_data, gen_data, bins=50):
    """Calculates the KL divergence between two distributions"""
    # Flatten data into 1D
    real_x = real_data[:, :, 0].flatten()
    real_y = real_data[:, :, 1].flatten()
    gen_x = gen_data[:, :, 0].flatten()
    gen_y = gen_data[:, :, 1].flatten()

    # Calculate KL divergence for X dimension
    hist_real_x, bin_edges = np.histogram(real_x, bins=bins, density=True)
    hist_gen_x, _ = np.histogram(gen_x, bins=bin_edges, density=True)
    # Avoid division by zero
    hist_real_x = np.clip(hist_real_x, 1e-10, None)
    hist_gen_x = np.clip(hist_gen_x, 1e-10, None)
    kl_x = entropy(hist_real_x, hist_gen_x)

    # Calculate KL divergence for Y dimension
    hist_real_y, bin_edges = np.histogram(real_y, bins=bins, density=True)
    hist_gen_y, _ = np.histogram(gen_y, bins=bin_edges, density=True)
    # Avoid division by zero
    hist_real_y = np.clip(hist_real_y, 1e-10, None)
    hist_gen_y = np.clip(hist_gen_y, 1e-10, None)
    kl_y = entropy(hist_real_y, hist_gen_y)

    # Return the average KL divergence for X and Y dimensions
    return (kl_x + kl_y) / 2

# Wasserstein Distance
def wasserstein_distance1(real_data, gen_data):
    """
    Calculates the 2-Wasserstein distance (Earth Mover's Distance) between two datasets

    Warning: For large datasets, the calculation cost is O(N*M), which may be slow or memory intensive
    """
    # Convert 3D data to 2D
    # (N, T, 2) -> (N, T*2) Flatten X and Y features of each sample into one row
    #  [x1, y1, x2, y2, ..., xT, yT]

    # If you want [x1, x2, ..., xT, y1, y2, ..., yT],
    # you should transpose first: real_data.transpose(0, 2, 1).reshape(real_data.shape[0], -1)

    real_flat = real_data.reshape(real_data.shape[0], -1)
    # What is the logic here? Is it [x1,y1, x2, y2, ... ,x100,y100] or [x1,x2,...,x100,y1,y2,...,y100]?
    #  [x1, y1, x2, y2, ..., xT, yT]
    gen_flat = gen_data.reshape(gen_data.shape[0], -1)

    # Uniform weights
    n = real_flat.shape[0]
    m = gen_flat.shape[0]
    w_real = np.ones(n) / n
    w_gen = np.ones(m) / m

    # Cost matrix: Pairwise Euclidean distance
    cost_matrix = ot.dist(real_flat, gen_flat, metric='euclidean')

    # Earth Mover's Distance^2
    emd2_value = ot.emd2(w_real, w_gen, cost_matrix)

    # Return the actual Wasserstein distance (square root of EMD^2)
    return np.sqrt(emd2_value)

# Wasserstein Distance
def wasserstein_distance2(real_data, gen_data):
    """
    Calculates the 2-Wasserstein distance (Earth Mover's Distance) between two datasets

    Warning: For large datasets, the calculation cost is O(N*M), which may be slow or memory intensive
    """
    # Convert 3D data to 2D
    # (N, T, 2) -> (N, T*2) Flatten X and Y features of each sample into one row


    # If you want [x1, x2, ..., xT, y1, y2, ..., yT],
    # you should transpose first:

    real_flat = real_data.transpose(0, 2, 1).reshape(real_data.shape[0], -1)

    gen_flat = gen_data.reshape(gen_data.shape[0], -1)

    # Uniform weights
    n = real_flat.shape[0]
    m = gen_flat.shape[0]
    w_real = np.ones(n) / n
    w_gen = np.ones(m) / m

    # Cost matrix: Pairwise Euclidean distance
    cost_matrix = ot.dist(real_flat, gen_flat, metric='euclidean')

    # Earth Mover's Distance^2
    emd2_value = ot.emd2(w_real, w_gen, cost_matrix)

    # Return the actual Wasserstein distance (square root of EMD^2)
    return np.sqrt(emd2_value)

def compare_datasets(real_data, gen_data, model_name="Generated Model"):
    """
    Compares real data with generated data, calculates various evaluation metrics

    Parameters:
    real_data: Real dataset, shape (N, T, 2)
    gen_data: Generated dataset, shape (M, T, 2)
    model_name: Model name, used for printing output
    """
    print(f"\n====== {model_name} Evaluation Results ======")


    # [New metric added here] Signature Distance
    # Default level 3
    sig_dist_val = path_signature_distance(real_data, gen_data, level=3)

    # Calculate and print evaluation metrics
    fnorm_diff = frobenius_corr_diff(real_data, gen_data)
    mse_val = mse_corr_diff(real_data, gen_data)
    mae_val = mae_corr_diff(real_data, gen_data)
    corr_val = corr_of_corrs(real_data, gen_data)
    # Note: wd_value is not defined before use. Assuming it should be one of wd_value1 or wd_value2.
    # Let's use wd_value1 for now, but this should be clarified.
    # For consistency with the plot, let's use wd_value1 as the 'wasserstein' metric.
    wd_value = wasserstein_distance1(real_data, gen_data) # Using wasserstein_distance1 as the general one
    marginal_wd = marginal_wasserstein_distance(real_data, gen_data)
    conditional_wd = conditional_wasserstein_distance(real_data, gen_data)
    kl_value = kl_divergence(real_data, gen_data)

    print(f"Frobenius Norm Difference: {fnorm_diff:.6f}")
    print(f"Correlation Matrix MSE: {mse_val:.6f}")
    print(f"Correlation Matrix MAE: {mae_val:.6f}")
    print(f"Pearson Correlation of Flattened Correlation Matrices: {corr_val:.6f}")
    print(f"General Wasserstein Distance: {wd_value:.6f}")
    print(f"Marginal Wasserstein Distance (MarginalWD): {marginal_wd:.6f}")
    print(f"Conditional Wasserstein Distance (ConditionalWD): {conditional_wd:.6f}")
    print(f"KL Divergence: {kl_value:.6f}")
    if sig_dist_val is not None and not np.isnan(sig_dist_val):
        print(f"Signature Distance (level 3): {sig_dist_val:.6f}")
    else:
        print(f"Signature Distance (level 3): Calculation failed or not performed")

    return {
        'frobenius_norm': fnorm_diff,
        'mse': mse_val,
        'mae': mae_val,
        'correlation': corr_val,
        'wasserstein': wd_value, # Using wd_value1
        'marginal_wasserstein': marginal_wd,
        'conditional_wasserstein': conditional_wd,
        'kl_divergence': kl_value,
        'signature_distance': sig_dist_val if sig_dist_val is not None else np.nan
    }

# =============== Main Function ===============
def main():
    """Main function: Loads data, calculates evaluation metrics, and visualizes"""
    print("Starting to load datasets...")
    real_data, diffwave_data, adaptive_diffwave_data, diffwave_model_name, adaptive_model_name, calibrated_data, ddpm_data, ddpm_model_name = load_datasets()

    # Create directory for saving charts
    import os
    save_dir = "evaluation_figures"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        print(f"Created chart save directory: {save_dir}")

    # Format model names for readability (capitalize first letter)
    # Names used for display in charts and output
    diffwave_display_name = diffwave_model_name.capitalize()
    adaptive_display_name = ' '.join(word.capitalize() for word in adaptive_model_name.split('_'))
    calibrated_display_name = 'Calibrated AR-GARCH'
    ddpm_display_name = ddpm_model_name.capitalize()

    # Format for filenames (use underscores instead of spaces)
    diffwave_file_name = diffwave_display_name
    adaptive_file_name = '_'.join(word.capitalize() for word in adaptive_model_name.split('_'))
    calibrated_file_name = 'calibrated_model'
    ddpm_file_name = ddpm_display_name

    # Calculate uniform data ranges
    print("\nCalculating uniform visualization ranges...")

    # Calculate uniform range for correlation matrices
    corr_matrices = [
        np.corrcoef(np.hstack([real_data[:, :, 0], real_data[:, :, 1]]), rowvar=False),
        np.corrcoef(np.hstack([diffwave_data[:, :, 0], diffwave_data[:, :, 1]]), rowvar=False),
        np.corrcoef(np.hstack([adaptive_diffwave_data[:, :, 0], adaptive_diffwave_data[:, :, 1]]), rowvar=False),
        np.corrcoef(np.hstack([calibrated_data[:, :, 0], calibrated_data[:, :, 1]]), rowvar=False),
        np.corrcoef(np.hstack([ddpm_data[:, :, 0], ddpm_data[:, :, 1]]), rowvar=False)
    ]
    corr_min = min(np.min(matrix) for matrix in corr_matrices)
    corr_max = max(np.max(matrix) for matrix in corr_matrices)
    print(f"Correlation matrix uniform range: [{corr_min:.4f}, {corr_max:.4f}]")

    # Calculate uniform range for covariance matrices
    cov_matrices = [
        np.cov(np.hstack([real_data[:, :, 0], real_data[:, :, 1]]), rowvar=False),
        np.cov(np.hstack([diffwave_data[:, :, 0], diffwave_data[:, :, 1]]), rowvar=False),
        np.cov(np.hstack([adaptive_diffwave_data[:, :, 0], adaptive_diffwave_data[:, :, 1]]), rowvar=False),
        np.cov(np.hstack([calibrated_data[:, :, 0], calibrated_data[:, :, 1]]), rowvar=False),
        np.cov(np.hstack([ddpm_data[:, :, 0], ddpm_data[:, :, 1]]), rowvar=False)
    ]
    cov_min = min(np.min(matrix) for matrix in cov_matrices)
    cov_max = max(np.max(matrix) for matrix in cov_matrices)
    print(f"Covariance matrix uniform range: [{cov_min:.4f}, {cov_max:.4f}]")

    # Calculate uniform range for statistical distribution plots
    # Mean
    means_x_all = [
        np.mean(real_data[:, :, 0], axis=1),
        np.mean(diffwave_data[:, :, 0], axis=1),
        np.mean(adaptive_diffwave_data[:, :, 0], axis=1),
        np.mean(calibrated_data[:, :, 0], axis=1),
        np.mean(ddpm_data[:, :, 0], axis=1)
    ]
    means_y_all = [
        np.mean(real_data[:, :, 1], axis=1),
        np.mean(diffwave_data[:, :, 1], axis=1),
        np.mean(adaptive_diffwave_data[:, :, 1], axis=1),
        np.mean(calibrated_data[:, :, 1], axis=1),
        np.mean(ddpm_data[:, :, 1], axis=1)
    ]
    mean_x_lim = (min(np.min(x) for x in means_x_all), max(np.max(x) for x in means_x_all))
    mean_y_lim = (min(np.min(y) for y in means_y_all), max(np.max(y) for y in means_y_all))

    # Variance
    vars_x_all = [
        np.var(real_data[:, :, 0], axis=1),
        np.var(diffwave_data[:, :, 0], axis=1),
        np.var(adaptive_diffwave_data[:, :, 0], axis=1),
        np.var(calibrated_data[:, :, 0], axis=1),
        np.var(ddpm_data[:, :, 0], axis=1)
    ]
    vars_y_all = [
        np.var(real_data[:, :, 1], axis=1),
        np.var(diffwave_data[:, :, 1], axis=1),
        np.var(adaptive_diffwave_data[:, :, 1], axis=1),
        np.var(calibrated_data[:, :, 1], axis=1),
        np.var(ddpm_data[:, :, 1], axis=1)
    ]
    var_x_lim = (min(np.min(x) for x in vars_x_all), max(np.max(x) for x in vars_x_all))
    var_y_lim = (min(np.min(y) for y in vars_y_all), max(np.max(y) for y in vars_y_all))

    # Skewness
    skew_x_all = [
        skew(real_data[:, :, 0], axis=1),
        skew(diffwave_data[:, :, 0], axis=1),
        skew(adaptive_diffwave_data[:, :, 0], axis=1),
        skew(calibrated_data[:, :, 0], axis=1),
        skew(ddpm_data[:, :, 0], axis=1)
    ]
    skew_y_all = [
        skew(real_data[:, :, 1], axis=1),
        skew(diffwave_data[:, :, 1], axis=1),
        skew(adaptive_diffwave_data[:, :, 1], axis=1),
        skew(calibrated_data[:, :, 1], axis=1),
        skew(ddpm_data[:, :, 1], axis=1)
    ]
    skew_x_lim = (min(np.min(x) for x in skew_x_all), max(np.max(x) for x in skew_x_all))
    skew_y_lim = (min(np.min(y) for y in skew_y_all), max(np.max(y) for y in skew_y_all))

    # Uniform x-axis ranges for statistical distribution plots
    stat_xlims = [mean_x_lim, mean_y_lim, var_x_lim, var_y_lim, skew_x_lim, skew_y_lim]
    print(f"Uniform range for statistical distribution plots calculated")

    # Calculate uniform range for data summary plots
    x_all = np.concatenate([
        real_data[:, :, 0].flatten(),
        diffwave_data[:, :, 0].flatten(),
        adaptive_diffwave_data[:, :, 0].flatten(),
        calibrated_data[:, :, 0].flatten(),
        ddpm_data[:, :, 0].flatten()
    ])
    y_all = np.concatenate([
        real_data[:, :, 1].flatten(),
        diffwave_data[:, :, 1].flatten(),
        adaptive_diffwave_data[:, :, 1].flatten(),
        calibrated_data[:, :, 1].flatten(),
        ddpm_data[:, :, 1].flatten()
    ])

    x_lim = (np.min(x_all), np.max(x_all))
    y_lim = (np.min(y_all), np.max(y_all))

    summary_xlims = [x_lim, y_lim, x_lim, y_lim]

    # Basic data statistics, using uniform range
    data_summary(real_data, "Real Data", save_dir=save_dir, xlims=summary_xlims)
    data_summary(diffwave_data, f"{diffwave_file_name} Data", save_dir=save_dir, xlims=summary_xlims)
    data_summary(adaptive_diffwave_data, f"{adaptive_file_name} Data", save_dir=save_dir, xlims=summary_xlims)
    data_summary(calibrated_data, f"{calibrated_file_name} Data", save_dir=save_dir, xlims=summary_xlims)
    data_summary(ddpm_data, f"{ddpm_file_name} Data", save_dir=save_dir, xlims=summary_xlims)

    # Merge statistical plots of all datasets
    summary_paths = [
        f"{save_dir}/Real_Data_summary.png",
        f"{save_dir}/{diffwave_file_name}_Data_summary.png",
        f"{save_dir}/{adaptive_file_name}_Data_summary.png",
        f"{save_dir}/{calibrated_file_name}_Data_summary.png",
        f"{save_dir}/{ddpm_file_name}_Data_summary.png"
    ]
    merge_images(summary_paths, f"{save_dir}/merged_data_summaries.png")
    print(f"Statistical plots of all datasets merged and saved to: {save_dir}/merged_data_summaries.png")

    # Visualize correlation matrices, using uniform range
    print("\nPlotting correlation matrices...")
    real_corr_path = f"{save_dir}/real_corr_matrix.png"
    diffwave_corr_path = f"{save_dir}/{diffwave_model_name}_corr_matrix.png"
    adaptive_corr_path = f"{save_dir}/{adaptive_model_name}_corr_matrix.png"
    calibrated_corr_path = f"{save_dir}/calibrated_model_corr_matrix.png"
    ddpm_corr_path = f"{save_dir}/{ddpm_model_name}_corr_matrix.png"

    plot_correlation_matrix(real_data, "Real Data Correlation Matrix", real_corr_path, vmin=corr_min, vmax=corr_max)
    plot_correlation_matrix(diffwave_data, f"{diffwave_display_name} Data Correlation Matrix", diffwave_corr_path, vmin=corr_min, vmax=corr_max)
    plot_correlation_matrix(adaptive_diffwave_data, f"{adaptive_display_name} Data Correlation Matrix", adaptive_corr_path, vmin=corr_min, vmax=corr_max)
    plot_correlation_matrix(calibrated_data, f"{calibrated_display_name} Data Correlation Matrix", calibrated_corr_path, vmin=corr_min, vmax=corr_max)
    plot_correlation_matrix(ddpm_data, f"{ddpm_display_name} Data Correlation Matrix", ddpm_corr_path, vmin=corr_min, vmax=corr_max)

    # Stitch correlation matrix plots
    corr_paths = [real_corr_path, diffwave_corr_path, adaptive_corr_path, calibrated_corr_path, ddpm_corr_path]
    merge_images(corr_paths, f"{save_dir}/merged_correlation_matrices.png")

    # Visualize covariance matrices, using uniform range
    print("\nPlotting covariance matrices...")
    real_cov_path = f"{save_dir}/real_cov_matrix.png"
    diffwave_cov_path = f"{save_dir}/{diffwave_model_name}_cov_matrix.png"
    adaptive_cov_path = f"{save_dir}/{adaptive_model_name}_cov_matrix.png"
    calibrated_cov_path = f"{save_dir}/calibrated_model_cov_matrix.png"
    ddpm_cov_path = f"{save_dir}/{ddpm_model_name}_cov_matrix.png"

    plot_covariance_matrix(real_data, "Real Data Covariance Matrix", real_cov_path, vmin=cov_min, vmax=cov_max)
    plot_covariance_matrix(diffwave_data, f"{diffwave_display_name} Data Covariance Matrix", diffwave_cov_path, vmin=cov_min, vmax=cov_max)
    plot_covariance_matrix(adaptive_diffwave_data, f"{adaptive_display_name} Data Covariance Matrix", adaptive_cov_path, vmin=cov_min, vmax=cov_max)
    plot_covariance_matrix(calibrated_data, f"{calibrated_display_name} Data Covariance Matrix", calibrated_cov_path, vmin=cov_min, vmax=cov_max)
    plot_covariance_matrix(ddpm_data, f"{ddpm_display_name} Data Covariance Matrix", ddpm_cov_path, vmin=cov_min, vmax=cov_max)

    # Stitch covariance matrix plots
    cov_paths = [real_cov_path, diffwave_cov_path, adaptive_cov_path, calibrated_cov_path, ddpm_cov_path]
    merge_images(cov_paths, f"{save_dir}/merged_covariance_matrices.png")

    # Visualize statistical distributions, using uniform range
    print("\nPlotting statistical distributions...")
    real_stats_path = f"{save_dir}/real_stats_dist.png"
    diffwave_stats_path = f"{save_dir}/{diffwave_model_name}_stats_dist.png"
    adaptive_stats_path = f"{save_dir}/{adaptive_model_name}_stats_dist.png"
    calibrated_stats_path = f"{save_dir}/calibrated_model_stats_dist.png"
    ddpm_stats_path = f"{save_dir}/{ddpm_model_name}_stats_dist.png"

    plot_statistical_distributions(real_data, "Real Data Statistical Distributions", real_stats_path, xlims=stat_xlims)
    plot_statistical_distributions(diffwave_data, f"{diffwave_display_name} Data Statistical Distributions", diffwave_stats_path, xlims=stat_xlims)
    plot_statistical_distributions(adaptive_diffwave_data, f"{adaptive_display_name} Data Statistical Distributions", adaptive_stats_path, xlims=stat_xlims)
    plot_statistical_distributions(calibrated_data, f"{calibrated_display_name} Data Statistical Distributions", calibrated_stats_path, xlims=stat_xlims)
    plot_statistical_distributions(ddpm_data, f"{ddpm_display_name} Data Statistical Distributions", ddpm_stats_path, xlims=stat_xlims)

    # Stitch statistical distribution plots
    stats_paths = [real_stats_path, diffwave_stats_path, adaptive_stats_path, calibrated_stats_path, ddpm_stats_path]
    merge_images(stats_paths, f"{save_dir}/merged_statistical_distributions.png")

    # Calculate evaluation metrics
    print("\nCalculating evaluation metrics...")
    diffwave_metrics = compare_datasets(real_data, diffwave_data, f"{diffwave_display_name} Model")
    adaptive_metrics = compare_datasets(real_data, adaptive_diffwave_data, f"{adaptive_display_name} Model")
    calibrated_metrics = compare_datasets(real_data, calibrated_data, f"{calibrated_display_name} Model")
    ddpm_metrics = compare_datasets(real_data, ddpm_data, f"{ddpm_display_name} Model")

    # Compare model performance
    print("\n====== Model Performance Comparison ======")
    metrics = ['frobenius_norm', 'mse', 'mae', 'correlation', 'wasserstein',
              'marginal_wasserstein', 'conditional_wasserstein', 'kl_divergence',
              'signature_distance']
    better_counts = {diffwave_display_name: 0, adaptive_display_name: 0, calibrated_display_name: 0, ddpm_display_name: 0}

    # Create performance comparison plot
    fig, axs = plt.subplots(5, 2, figsize=(16, 30))
    bar_width = 0.18

    # Define an offset list for each model
    offsets = np.array([-1.5, -0.5, 0.5, 1.5]) * bar_width

    # Helper function to plot a metric for all models
    def plot_metric(ax, metric_name, metrics_dict, display_names, colors):
        index = np.arange(1)
        values = [metrics_dict[name].get(metric_name, np.nan) for name in display_names]
        for i, val in enumerate(values):
            if not np.isnan(val):
                ax.bar(index + offsets[i], val, bar_width, label=display_names[i], color=colors[i], alpha=0.7)
        ax.set_xlabel('Metric')
        ax.set_ylabel('Value')
        ax.set_title(f'{metric_name.replace("_", " ").capitalize()} Comparison')
        ax.set_xticks(index)
        ax.set_xticklabels([metric_name.replace("_", " ").capitalize()])
        ax.legend()

    model_display_names = [diffwave_display_name, adaptive_display_name, calibrated_display_name, ddpm_display_name]
    model_metrics = {
        diffwave_display_name: diffwave_metrics,
        adaptive_display_name: adaptive_metrics,
        calibrated_display_name: calibrated_metrics,
        ddpm_display_name: ddpm_metrics
    }
    model_colors = ['blue', 'red', 'green', 'purple']

    # Plotting each metric
    plot_metric(axs[0, 0], 'frobenius_norm', model_metrics, model_display_names, model_colors)
    plot_metric(axs[0, 1], 'mse', model_metrics, model_display_names, model_colors)
    plot_metric(axs[1, 0], 'mae', model_metrics, model_display_names, model_colors)
    plot_metric(axs[1, 1], 'correlation', model_metrics, model_display_names, model_colors)
    plot_metric(axs[2, 0], 'wasserstein', model_metrics, model_display_names, model_colors)
    plot_metric(axs[2, 1], 'kl_divergence', model_metrics, model_display_names, model_colors)
    plot_metric(axs[3, 0], 'marginal_wasserstein', model_metrics, model_display_names, model_colors)
    plot_metric(axs[3, 1], 'conditional_wasserstein', model_metrics, model_display_names, model_colors)
    plot_metric(axs[4, 0], 'signature_distance', model_metrics, model_display_names, model_colors)

    # Can hide the last empty subplot (axs[4,1])
    axs[4, 1].axis('off')

    plt.tight_layout()
    plt.savefig(f"{save_dir}/model_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Model performance comparison plot saved to: {save_dir}/model_comparison.png")

    # Compare performance for each metric
    for metric in metrics:
        # Skip comparison if all values are NaN
        all_nan = all(np.isnan(model_metrics[name].get(metric, np.nan)) for name in model_display_names)
        if all_nan:
            print(f"{metric}: All model values are NaN, skipping comparison.")
            continue

        values = []
        models_for_metric = []
        for name in model_display_names:
            val = model_metrics[name].get(metric, np.nan)
            if not np.isnan(val):
                values.append(val)
                models_for_metric.append(name)

        if not values: # If all values were NaN after checking
             print(f"{metric}: All model values are NaN or not calculated.")
             continue

        # Determine the best model for the metric
        if metric == 'correlation':  # Higher is better for correlation
            best_val_idx = np.argmax(values)
        else:  # Lower is better for other metrics
            best_val_idx = np.argmin(values)

        best_model = models_for_metric[best_val_idx]
        better_counts[best_model] += 1

        print(f"{metric}: " + ", ".join([f"{name}={model_metrics[name].get(metric, np.nan):.6f}" for name in model_display_names]) + f", Best model: {best_model}")

    print(f"\nSummary: " + ", ".join([f"{name} better metrics count: {better_counts[name]}" for name in model_display_names]))

    winner = max(better_counts, key=better_counts.get)
    print(f"Overall best performing model: {winner}")

    # Save results to a text file
    with open(f"{save_dir}/evaluation_results.txt", "w") as f:
        f.write("====== Model Performance Evaluation Results ======\n\n")
        for name in model_display_names:
            f.write(f"{name} Model Evaluation Results:\n")
            for metric, value in model_metrics[name].items():
                f.write(f"{metric}: {value:.6f}\n")
            f.write("\n")

        f.write(f"Summary: " + ", ".join([f"{name} better metrics count: {better_counts[name]}" for name in model_display_names]) + "\n")
        f.write(f"Overall best performing model: {winner}\n")

    print(f"Evaluation results saved to: {save_dir}/evaluation_results.txt")

if __name__ == "__main__":
    main()
