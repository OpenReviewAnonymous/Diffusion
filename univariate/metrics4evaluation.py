#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Evaluation metric calculation and visualization for time series generative models
"""

# =============== Import necessary libraries ===============
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew
from sklearn.neighbors import KernelDensity
import ot  # Required: pip install pot
from tqdm import tqdm
from datetime import datetime
from PIL import Image
timestamp = datetime.now().strftime("%Y-%m-%d_%H:%M")


# Import from diffwave
try:
    from diffwave import N
    from diffwave import load_data
    N_test = int(0.12 * N)
except ImportError:
    print("Warning: Unable to import diffwave module")

# =============== Data loading function ===============
def load_datasets():
    """Load all experimental datasets"""
    # Real data
    real_data = np.load("test_samples_normalized_2025-03-28_05:11.npy")
    print("Real data shape:", real_data.shape)
    
    # Data generated by DiffWave model
    diffwave_data = np.load("samples_diffwave_2025-03-28_04:59.npy")
    print("DiffWave data shape:", diffwave_data.shape)
    
    # Data generated by Adaptive DiffWave model
    adaptive_diffwave_data = np.load("samples_adaptive_2025-03-28_04-58.npy")
    print("Adaptive DiffWave data shape:", adaptive_diffwave_data.shape)
    
    return real_data, diffwave_data, adaptive_diffwave_data

# =============== Data statistics and visualization functions ===============
def plot_correlation_matrix(data, title="Correlation Matrix", save_path=None, vmin=-1, vmax=1):
    """
    Plot the correlation matrix heatmap for 2D time series data
    
    Args:
    data: Data of shape (N, T, 2), where N is the number of samples, T is the time steps, 2 is the dimension
    title: Chart title
    save_path: Path to save the chart, if None use the title as filename
    vmin, vmax: Color range for the heatmap
    """
    # Extract X and Y sequences from (N, T, 2)
    N, T, dim = data.shape
    X_flat = data[:, :, 0]  # Extract first dimension data, shape (N, T)
    Y_flat = data[:, :, 1]  # Extract second dimension data, shape (N, T)
    
    # Concatenate X and Y to shape (N, 2T)
    flatten_data = np.hstack([X_flat, Y_flat])
    
    # Compute correlation matrix, rowvar=False means each column is a variable
    corr_matrix = np.corrcoef(flatten_data, rowvar=False)
    
    # Plot correlation matrix heatmap
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        corr_matrix,
        cmap="RdBu_r",  # Red-blue contrast color
        center=0,       # Set 0 as color center
        vmax=vmax, vmin=vmin,  # Use the given unified range
        square=True,
        fmt='.2f',
        cbar=True
    )
    
    # Draw dividing lines at T to split the matrix into four blocks
    plt.axvline(x=T, color='k', linestyle='--')  # Vertical split
    plt.axhline(y=T, color='k', linestyle='--')  # Horizontal split
    
    # Add block labels
    plt.text(T/2, -5, "X-X", ha='center')
    plt.text(T+T/2, -5, "X-Y", ha='center')
    plt.text(-5, T/2, "Y-X", va='center', rotation=90)
    plt.text(-5, T+T/2, "Y-Y", va='center', rotation=90)
    
    plt.title(title)
    plt.tight_layout()
    
    # Save chart
    if save_path is None:
        save_path = f"{title.replace(' ', '_')}.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Correlation matrix plot saved to: {save_path}")
    return corr_matrix

def plot_covariance_matrix(data, title="Covariance Matrix", save_path=None, vmin=None, vmax=None):
    """
    Plot the covariance matrix heatmap for 2D time series data
    
    Args:
    data: Data of shape (N, T, 2), where N is the number of samples, T is the time steps, 2 is the dimension
    title: Chart title
    save_path: Path to save the chart, if None use the title as filename
    vmin, vmax: Color range for the heatmap
    """
    # Extract X and Y sequences from (N, T, 2)
    N, T, dim = data.shape
    X_flat = data[:, :, 0]  # Extract first dimension data, shape (N, T)
    Y_flat = data[:, :, 1]  # Extract second dimension data, shape (N, T)
    
    # Concatenate X and Y to shape (N, 2T)
    flatten_data = np.hstack([X_flat, Y_flat])
    
    # Compute covariance matrix
    cov_matrix = np.cov(flatten_data, rowvar=False)
    
    # Plot covariance matrix heatmap
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        cov_matrix,
        cmap="RdBu_r",  # Red-blue contrast color
        center=0,       # Set 0 as color center
        vmin=vmin, vmax=vmax,  # Use the given unified range
        square=True,
        fmt='.2f',
        cbar=True
    )
    
    # Draw dividing lines at T to split the matrix into four blocks
    plt.axvline(x=T, color='k', linestyle='--')  # Vertical split
    plt.axhline(y=T, color='k', linestyle='--')  # Horizontal split
    
    # Add block labels
    plt.text(T/2, -5, "X-X", ha='center')
    plt.text(T+T/2, -5, "X-Y", ha='center')
    plt.text(-5, T/2, "Y-X", va='center', rotation=90)
    plt.text(-5, T+T/2, "Y-Y", va='center', rotation=90)
    
    plt.title(title)
    plt.tight_layout()
    
    # Save chart
    if save_path is None:
        save_path = f"{title.replace(' ', '_')}.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Covariance matrix plot saved to: {save_path}")
    return cov_matrix

def plot_statistical_distributions(data, title="Statistical Distributions", save_path=None, xlims=None, ylims=None):
    """
    Plot statistical distributions for 2D time series data
    
    Args:
    data: Data of shape (N, T, 2), where N is the number of samples, T is the time steps, 2 is the dimension
    title: Chart title
    save_path: Path to save the chart, if None use the title as filename
    xlims: List of x-axis ranges for each subplot, format [(xmin1, xmax1), (xmin2, xmax2), ...]
    ylims: List of y-axis ranges for each subplot, format [(ymin1, ymax1), (ymin2, ymax2), ...]
    """
    N, T, dim = data.shape
    
    # Compute statistics
    means_x = np.mean(data[:, :, 0], axis=1)  # Mean of X dimension
    means_y = np.mean(data[:, :, 1], axis=1)  # Mean of Y dimension
    
    vars_x = np.var(data[:, :, 0], axis=1)    # Variance of X dimension
    vars_y = np.var(data[:, :, 1], axis=1)    # Variance of Y dimension
    
    skew_x = skew(data[:, :, 0], axis=1)      # Skewness of X dimension
    skew_y = skew(data[:, :, 1], axis=1)      # Skewness of Y dimension
    
    # Create a 3x2 subplot layout (3 statistics x 2 dimensions)
    fig, axes = plt.subplots(3, 2, figsize=(15, 12))
    
    # Mean distribution
    axes[0, 0].hist(means_x, bins=50, color='blue', alpha=0.7)
    axes[0, 0].set_title('X Dimension Mean Distribution')
    axes[0, 0].set_xlabel('Mean')
    axes[0, 0].set_ylabel('Frequency')
    
    axes[0, 1].hist(means_y, bins=50, color='red', alpha=0.7)
    axes[0, 1].set_title('Y Dimension Mean Distribution')
    axes[0, 1].set_xlabel('Mean')
    axes[0, 1].set_ylabel('Frequency')
    
    # Variance distribution
    axes[1, 0].hist(vars_x, bins=50, color='blue', alpha=0.7)
    axes[1, 0].set_title('X Dimension Variance Distribution')
    axes[1, 0].set_xlabel('Variance')
    axes[1, 0].set_ylabel('Frequency')
    
    axes[1, 1].hist(vars_y, bins=50, color='red', alpha=0.7)
    axes[1, 1].set_title('Y Dimension Variance Distribution')
    axes[1, 1].set_xlabel('Variance')
    axes[1, 1].set_ylabel('Frequency')
    
    # Skewness distribution
    axes[2, 0].hist(skew_x, bins=50, color='blue', alpha=0.7)
    axes[2, 0].set_title('X Dimension Skewness Distribution')
    axes[2, 0].set_xlabel('Skewness')
    axes[2, 0].set_ylabel('Frequency')
    
    axes[2, 1].hist(skew_y, bins=50, color='red', alpha=0.7)
    axes[2, 1].set_title('Y Dimension Skewness Distribution')
    axes[2, 1].set_xlabel('Skewness')
    axes[2, 1].set_ylabel('Frequency')
    
    # Set unified axis ranges
    if xlims is not None:
        for i in range(3):
            for j in range(2):
                idx = i * 2 + j
                if idx < len(xlims) and xlims[idx] is not None:
                    axes[i, j].set_xlim(xlims[idx])
    
    if ylims is not None:
        for i in range(3):
            for j in range(2):
                idx = i * 2 + j
                if idx < len(ylims) and ylims[idx] is not None:
                    axes[i, j].set_ylim(ylims[idx])
    
    plt.suptitle(title, fontsize=16)
    plt.tight_layout()
    plt.subplots_adjust(top=0.92)  # Leave space for the main title
    
    # Save chart
    if save_path is None:
        save_path = f"{title.replace(' ', '_')}.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Statistical distribution plot saved to: {save_path}")
    return (means_x, means_y), (vars_x, vars_y), (skew_x, skew_y)

def data_summary(data, name="Dataset", save_path=None, save_dir="evaluation_figures", xlims=None, ylims=None):
    """Print basic statistics of the dataset, considering 2D data, and generate visualization charts"""
    print(f"\n====== {name} Statistics ======")
    print(f"Shape: {data.shape}")
    
    # X dimension statistics
    x_data = data[:, :, 0].flatten()
    print(f"X Dimension:")
    print(f"  Min: {x_data.min():.4f}")
    print(f"  1% Quantile: {np.quantile(x_data, 0.01):.4f}")
    print(f"  25% Quantile: {np.quantile(x_data, 0.25):.4f}")
    print(f"  75% Quantile: {np.quantile(x_data, 0.75):.4f}")
    print(f"  99% Quantile: {np.quantile(x_data, 0.99):.4f}")
    print(f"  Max: {x_data.max():.4f}")
    
    # Y dimension statistics
    y_data = data[:, :, 1].flatten()
    print(f"Y Dimension:")
    print(f"  Min: {y_data.min():.4f}")
    print(f"  1% Quantile: {np.quantile(y_data, 0.01):.4f}")
    print(f"  25% Quantile: {np.quantile(y_data, 0.25):.4f}")
    print(f"  75% Quantile: {np.quantile(y_data, 0.75):.4f}")
    print(f"  99% Quantile: {np.quantile(y_data, 0.99):.4f}")
    print(f"  Max: {y_data.max():.4f}")
    
    # Create visualization charts
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Histogram
    axes[0, 0].hist(x_data, bins=50, color='blue', alpha=0.7)
    axes[0, 0].set_title('X Dimension Distribution Histogram')
    axes[0, 0].set_xlabel('Value')
    axes[0, 0].set_ylabel('Frequency')
    
    axes[0, 1].hist(y_data, bins=50, color='red', alpha=0.7)
    axes[0, 1].set_title('Y Dimension Distribution Histogram')
    axes[0, 1].set_xlabel('Value')
    axes[0, 1].set_ylabel('Frequency')
    
    # Box plot
    axes[1, 0].boxplot(x_data, vert=False, widths=0.7)
    axes[1, 0].set_title('X Dimension Box Plot')
    axes[1, 0].set_xlabel('Value')
    
    axes[1, 1].boxplot(y_data, vert=False, widths=0.7)
    axes[1, 1].set_title('Y Dimension Box Plot')
    axes[1, 1].set_xlabel('Value')
    
    # Set unified axis ranges
    if xlims is not None:
        for i in range(2):
            for j in range(2):
                idx = i * 2 + j
                if idx < len(xlims) and xlims[idx] is not None:
                    axes[i, j].set_xlim(xlims[idx])
    
    if ylims is not None:
        for i in range(2):
            for j in range(2):
                idx = i * 2 + j
                if idx < len(ylims) and ylims[idx] is not None:
                    axes[i, j].set_ylim(ylims[idx])
    
    plt.suptitle(f"{name} Statistical Distribution", fontsize=16)
    plt.tight_layout()
    plt.subplots_adjust(top=0.9)  # Make room for the title
    # Save chart
    if save_path is None:
        save_path = f"{save_dir}/{name.replace(' ', '_')}_summary.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Data statistics plot saved to: {save_path}")
    
    # Merge the statistics plots of the three datasets
    if name == "Adaptive DiffWave Data":  # When processing the last dataset
        summary_paths = [
            f"{save_dir}/Real_Data_summary.png",
            f"{save_dir}/DiffWave_Data_summary.png",
            f"{save_dir}/Adaptive_DiffWave_Data_summary.png"
        ]
        merge_images(summary_paths, f"{save_dir}/merged_data_summaries.png")
        print(f"Statistics plots of the three datasets have been merged and saved to: {save_dir}/merged_data_summaries.png")

# Image merging function
def merge_images(image_paths, output_path, border_size=5, border_color=(255, 0, 0)):
    """
    Horizontally merge multiple images and add borders
    
    Args:
    image_paths: List of image paths
    output_path: Output image path
    border_size: Border width
    border_color: Border color, RGB tuple
    """
    # Open all images
    images = [Image.open(p) for p in image_paths]
    
    # Get the size of the first image
    width, height = images[0].size
    
    # Set layout: 1 row, n columns
    rows = 1
    cols = len(images)
    
    # Add border to images
    def add_border(image, border_size=5, color=(255, 0, 0)):
        """Add a border of specified color and width to a single image"""
        w, h = image.size
        # Create a new image slightly larger than the original to hold the border
        bordered_img = Image.new("RGB", (w + 2*border_size, h + 2*border_size), color)
        # Paste the original image in the center of the new image
        bordered_img.paste(image, (border_size, border_size))
        return bordered_img
    
    # Add borders
    images_with_border = [add_border(img, border_size=border_size, color=border_color) for img in images]
    
    # Get the size of the images after adding borders
    bordered_width, bordered_height = images_with_border[0].size
    
    # Create the final merged image canvas
    combined_width = cols * bordered_width
    combined_height = rows * bordered_height
    merged_image = Image.new('RGB', (combined_width, combined_height))
    
    # Paste each sub-image in order into the large image
    for idx, img in enumerate(images_with_border):
        # Calculate which column the current image should be in
        col = idx % cols
        
        # Calculate the top-left pixel coordinates for pasting
        x_offset = col * bordered_width
        y_offset = 0  # Only one row, so y offset is 0
        
        # Paste the image at the specified position
        merged_image.paste(img, (x_offset, y_offset))
    
    # Save the final merged image
    merged_image.save(output_path)
    print(f"Merging complete, saved as {output_path}")
    return output_path

# =============== Evaluation metric calculation functions ===============
def correlation_matrix_difference(data1, data2):
    """
    Calculate the Frobenius norm of the difference between the correlation matrices of two 2D datasets
    
    Args:
    data1, data2: Datasets of shape (N, T, 2) and (M, T, 2)
    
    Returns:
    float: Frobenius norm of the difference between the correlation matrices
    """
    # Process data1
    N1, T, _ = data1.shape
    X1_flat = data1[:, :, 0]
    Y1_flat = data1[:, :, 1]
    flatten_data1 = np.hstack([X1_flat, Y1_flat])
    corr1 = np.corrcoef(flatten_data1, rowvar=False)
    
    # Process data2
    N2, T, _ = data2.shape
    X2_flat = data2[:, :, 0]
    Y2_flat = data2[:, :, 1]
    flatten_data2 = np.hstack([X2_flat, Y2_flat])
    corr2 = np.corrcoef(flatten_data2, rowvar=False)
    
    # Calculate Frobenius norm of the difference
    diff = corr1 - corr2
    return np.linalg.norm(diff, ord='fro')  # Frobenius norm

def compare_datasets(real_data, gen_data, model_name="Generative Model"):
    """
    Compare real data and generated data, calculate multiple evaluation metrics
    
    Args:
    real_data: Real dataset, shape (N, T, 2)
    gen_data: Generated dataset, shape (M, T, 2)
    model_name: Model name, used for print output
    """
    print(f"\n====== {model_name} Evaluation Results ======")
    
    # Frobenius norm difference
    def frobenius_corr_diff(real_data, gen_data):
        """Calculate Frobenius norm of the difference between correlation matrices"""
        # Process data1
        N1, T, _ = real_data.shape
        X1_flat = real_data[:, :, 0]
        Y1_flat = real_data[:, :, 1]
        flatten_data1 = np.hstack([X1_flat, Y1_flat])
        corr_real = np.corrcoef(flatten_data1, rowvar=False)
        
        # Process data2
        N2, T, _ = gen_data.shape
        X2_flat = gen_data[:, :, 0]
        Y2_flat = gen_data[:, :, 1]
        flatten_data2 = np.hstack([X2_flat, Y2_flat])
        corr_gen = np.corrcoef(flatten_data2, rowvar=False)
        
        diff = corr_real - corr_gen
        return np.linalg.norm(diff, ord='fro')

    # Mean squared error
    def mse_corr_diff(real_data, gen_data):
        """Calculate element-wise mean squared error (MSE) of correlation matrices"""
        # Process data1
        N1, T, _ = real_data.shape
        X1_flat = real_data[:, :, 0]
        Y1_flat = real_data[:, :, 1]
        flatten_data1 = np.hstack([X1_flat, Y1_flat])
        corr_real = np.corrcoef(flatten_data1, rowvar=False)
        
        # Process data2
        N2, T, _ = gen_data.shape
        X2_flat = gen_data[:, :, 0]
        Y2_flat = gen_data[:, :, 1]
        flatten_data2 = np.hstack([X2_flat, Y2_flat])
        corr_gen = np.corrcoef(flatten_data2, rowvar=False)
        
        diff = corr_real - corr_gen
        return np.mean(diff**2)

    # Mean absolute error
    def mae_corr_diff(real_data, gen_data):
        """Calculate element-wise mean absolute error (MAE) of correlation matrices"""
        # Process data1
        N1, T, _ = real_data.shape
        X1_flat = real_data[:, :, 0]
        Y1_flat = real_data[:, :, 1]
        flatten_data1 = np.hstack([X1_flat, Y1_flat])
        corr_real = np.corrcoef(flatten_data1, rowvar=False)
        
        # Process data2
        N2, T, _ = gen_data.shape
        X2_flat = gen_data[:, :, 0]
        Y2_flat = gen_data[:, :, 1]
        flatten_data2 = np.hstack([X2_flat, Y2_flat])
        corr_gen = np.corrcoef(flatten_data2, rowvar=False)
        
        diff = corr_real - corr_gen
        return np.mean(np.abs(diff))

    # Correlation coefficient of correlation matrices
    def corr_of_corrs(real_data, gen_data):
        """Calculate Pearson correlation coefficient between flattened correlation matrices"""
        # Process data1
        N1, T, _ = real_data.shape
        X1_flat = real_data[:, :, 0]
        Y1_flat = real_data[:, :, 1]
        flatten_data1 = np.hstack([X1_flat, Y1_flat])
        corr_real = np.corrcoef(flatten_data1, rowvar=False)
        
        # Process data2
        N2, T, _ = gen_data.shape
        X2_flat = gen_data[:, :, 0]
        Y2_flat = gen_data[:, :, 1]
        flatten_data2 = np.hstack([X2_flat, Y2_flat])
        corr_gen = np.corrcoef(flatten_data2, rowvar=False)
        
        # Flatten to vectors
        vec_real = corr_real.flatten()
        vec_gen = corr_gen.flatten()
        
        # Calculate Pearson correlation coefficient
        r = np.corrcoef(vec_real, vec_gen)[0, 1]
        return r

    # Wasserstein distance
    def wasserstein_distance(real_data, gen_data):
        """
        Calculate the 2-Wasserstein distance (Earth Mover's distance) between two datasets
        
        Warning: For large datasets, computation cost is O(N*M), may be slow or memory intensive
        """
        # Convert 3D data to 2D
        # (N, T, 2) -> (N, T*2), flatten X and Y features of each sample into one row
        real_flat = real_data.reshape(real_data.shape[0], -1)
        gen_flat = gen_data.reshape(gen_data.shape[0], -1)
        
        # Uniform weights
        n = real_flat.shape[0]
        m = gen_flat.shape[0]
        w_real = np.ones(n) / n
        w_gen = np.ones(m) / m
        
        # Cost matrix: pairwise Euclidean distances
        cost_matrix = ot.dist(real_flat, gen_flat, metric='euclidean')
        
        # Earth Mover's Distance^2
        emd2_value = ot.emd2(w_real, w_gen, cost_matrix)
        
        # Return actual Wasserstein distance (sqrt of EMD^2)
        return np.sqrt(emd2_value)

    # Calculate and print evaluation metrics
    fnorm_diff = frobenius_corr_diff(real_data, gen_data)
    mse_val = mse_corr_diff(real_data, gen_data)
    mae_val = mae_corr_diff(real_data, gen_data)
    corr_val = corr_of_corrs(real_data, gen_data)
    wd_value = wasserstein_distance(real_data, gen_data)
    
    print(f"Frobenius norm difference: {fnorm_diff:.6f}")
    print(f"Correlation matrix MSE: {mse_val:.6f}")
    print(f"Correlation matrix MAE: {mae_val:.6f}")
    print(f"Pearson correlation of flattened correlation matrices: {corr_val:.6f}")
    print(f"Wasserstein distance: {wd_value:.6f}")
    
    return {
        'frobenius_norm': fnorm_diff,
        'mse': mse_val,
        'mae': mae_val,
        'correlation': corr_val,
        'wasserstein': wd_value
    }

# =============== Main function ===============
def main():
    """Main function: load data, compute evaluation metrics, and visualize"""
    print("Start loading datasets...")
    real_data, diffwave_data, adaptive_diffwave_data = load_datasets()
    
    # Create directory to save charts
    import os
    save_dir = "evaluation_figures"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        print(f"Created chart save directory: {save_dir}")
    
    # Compute unified data ranges
    print("\nComputing unified visualization ranges...")
    
    # Compute unified range for correlation matrices
    corr_matrices = [
        np.corrcoef(np.hstack([real_data[:, :, 0], real_data[:, :, 1]]), rowvar=False),
        np.corrcoef(np.hstack([diffwave_data[:, :, 0], diffwave_data[:, :, 1]]), rowvar=False),
        np.corrcoef(np.hstack([adaptive_diffwave_data[:, :, 0], adaptive_diffwave_data[:, :, 1]]), rowvar=False)
    ]
    corr_min = min(np.min(matrix) for matrix in corr_matrices)
    corr_max = max(np.max(matrix) for matrix in corr_matrices)
    print(f"Unified range for correlation matrices: [{corr_min:.4f}, {corr_max:.4f}]")
    
    # Compute unified range for covariance matrices
    cov_matrices = [
        np.cov(np.hstack([real_data[:, :, 0], real_data[:, :, 1]]), rowvar=False),
        np.cov(np.hstack([diffwave_data[:, :, 0], diffwave_data[:, :, 1]]), rowvar=False),
        np.cov(np.hstack([adaptive_diffwave_data[:, :, 0], adaptive_diffwave_data[:, :, 1]]), rowvar=False)
    ]
    cov_min = min(np.min(matrix) for matrix in cov_matrices)
    cov_max = max(np.max(matrix) for matrix in cov_matrices)
    print(f"Unified range for covariance matrices: [{cov_min:.4f}, {cov_max:.4f}]")
    
    # Compute unified range for statistical distribution plots
    # Mean
    means_x_all = [
        np.mean(real_data[:, :, 0], axis=1),
        np.mean(diffwave_data[:, :, 0], axis=1),
        np.mean(adaptive_diffwave_data[:, :, 0], axis=1)
    ]
    means_y_all = [
        np.mean(real_data[:, :, 1], axis=1),
        np.mean(diffwave_data[:, :, 1], axis=1),
        np.mean(adaptive_diffwave_data[:, :, 1], axis=1)
    ]
    mean_x_lim = (min(np.min(x) for x in means_x_all), max(np.max(x) for x in means_x_all))
    mean_y_lim = (min(np.min(y) for y in means_y_all), max(np.max(y) for y in means_y_all))
    
    # Variance
    vars_x_all = [
        np.var(real_data[:, :, 0], axis=1),
        np.var(diffwave_data[:, :, 0], axis=1),
        np.var(adaptive_diffwave_data[:, :, 0], axis=1)
    ]
    vars_y_all = [
        np.var(real_data[:, :, 1], axis=1),
        np.var(diffwave_data[:, :, 1], axis=1),
        np.var(adaptive_diffwave_data[:, :, 1], axis=1)
    ]
    var_x_lim = (min(np.min(x) for x in vars_x_all), max(np.max(x) for x in vars_x_all))
    var_y_lim = (min(np.min(y) for y in vars_y_all), max(np.max(y) for y in vars_y_all))
    
    # Skewness
    skew_x_all = [
        skew(real_data[:, :, 0], axis=1),
        skew(diffwave_data[:, :, 0], axis=1),
        skew(adaptive_diffwave_data[:, :, 0], axis=1)
    ]
    skew_y_all = [
        skew(real_data[:, :, 1], axis=1),
        skew(diffwave_data[:, :, 1], axis=1),
        skew(adaptive_diffwave_data[:, :, 1], axis=1)
    ]
    skew_x_lim = (min(np.min(x) for x in skew_x_all), max(np.max(x) for x in skew_x_all))
    skew_y_lim = (min(np.min(y) for y in skew_y_all), max(np.max(y) for y in skew_y_all))
    
    # Unified x-axis range for statistical distribution plots
    stat_xlims = [mean_x_lim, mean_y_lim, var_x_lim, var_y_lim, skew_x_lim, skew_y_lim]
    print(f"Unified range for statistical distribution plots computed")
    
    # Compute unified range for data summary plots
    x_all = np.concatenate([
        real_data[:, :, 0].flatten(),
        diffwave_data[:, :, 0].flatten(),
        adaptive_diffwave_data[:, :, 0].flatten()
    ])
    y_all = np.concatenate([
        real_data[:, :, 1].flatten(),
        diffwave_data[:, :, 1].flatten(),
        adaptive_diffwave_data[:, :, 1].flatten()
    ])
    
    x_lim = (np.min(x_all), np.max(x_all))
    y_lim = (np.min(y_all), np.max(y_all))
    
    summary_xlims = [x_lim, y_lim, x_lim, y_lim]
    
    # Basic data statistics, using unified range
    data_summary(real_data, "Real Data", save_dir=save_dir, xlims=summary_xlims)
    data_summary(diffwave_data, "DiffWave Data", save_dir=save_dir, xlims=summary_xlims)
    data_summary(adaptive_diffwave_data, "Adaptive DiffWave Data", save_dir=save_dir, xlims=summary_xlims)
    
    # Visualize correlation matrices, using unified range
    print("\nPlotting correlation matrices...")
    real_corr_path = f"{save_dir}/real_corr_matrix.png"
    diffwave_corr_path = f"{save_dir}/diffwave_corr_matrix.png"
    adaptive_corr_path = f"{save_dir}/adaptive_diffwave_corr_matrix.png"
    
    plot_correlation_matrix(real_data, "Real Data Correlation Matrix", real_corr_path, vmin=corr_min, vmax=corr_max)
    plot_correlation_matrix(diffwave_data, "DiffWave Data Correlation Matrix", diffwave_corr_path, vmin=corr_min, vmax=corr_max)
    plot_correlation_matrix(adaptive_diffwave_data, "Adaptive DiffWave Data Correlation Matrix", adaptive_corr_path, vmin=corr_min, vmax=corr_max)
    
    # Merge correlation matrix plots
    corr_paths = [real_corr_path, diffwave_corr_path, adaptive_corr_path]
    merge_images(corr_paths, f"{save_dir}/merged_correlation_matrices.png")
    
    # Visualize covariance matrices, using unified range
    print("\nPlotting covariance matrices...")
    real_cov_path = f"{save_dir}/real_cov_matrix.png"
    diffwave_cov_path = f"{save_dir}/diffwave_cov_matrix.png"
    adaptive_cov_path = f"{save_dir}/adaptive_diffwave_cov_matrix.png"
    
    plot_covariance_matrix(real_data, "Real Data Covariance Matrix", real_cov_path, vmin=cov_min, vmax=cov_max)
    plot_covariance_matrix(diffwave_data, "DiffWave Data Covariance Matrix", diffwave_cov_path, vmin=cov_min, vmax=cov_max)
    plot_covariance_matrix(adaptive_diffwave_data, "Adaptive DiffWave Data Covariance Matrix", adaptive_cov_path, vmin=cov_min, vmax=cov_max)
    
    # Merge covariance matrix plots
    cov_paths = [real_cov_path, diffwave_cov_path, adaptive_cov_path]
    merge_images(cov_paths, f"{save_dir}/merged_covariance_matrices.png")
    
    # Visualize statistical distributions, using unified range
    print("\nPlotting statistical distributions...")
    real_stats_path = f"{save_dir}/real_stats_dist.png"
    diffwave_stats_path = f"{save_dir}/diffwave_stats_dist.png"
    adaptive_stats_path = f"{save_dir}/adaptive_diffwave_stats_dist.png"
    
    plot_statistical_distributions(real_data, "Real Data Statistical Distributions", real_stats_path, xlims=stat_xlims)
    plot_statistical_distributions(diffwave_data, "DiffWave Data Statistical Distributions", diffwave_stats_path, xlims=stat_xlims)
    plot_statistical_distributions(adaptive_diffwave_data, "Adaptive DiffWave Data Statistical Distributions", adaptive_stats_path, xlims=stat_xlims)
    
    # Merge statistical distribution plots
    stats_paths = [real_stats_path, diffwave_stats_path, adaptive_stats_path]
    merge_images(stats_paths, f"{save_dir}/merged_statistical_distributions.png")
    
    # Compute evaluation metrics
    print("\nComputing evaluation metrics...")
    diffwave_metrics = compare_datasets(real_data, diffwave_data, "DiffWave Model")
    adaptive_metrics = compare_datasets(real_data, adaptive_diffwave_data, "Adaptive DiffWave Model")
    
    # Compare the performance of the two models
    print("\n====== Model Performance Comparison ======")
    metrics = ['frobenius_norm', 'mse', 'mae', 'correlation', 'wasserstein']
    better_counts = {'DiffWave': 0, 'Adaptive DiffWave': 0}
    
    # Create 2x2 subplot for performance comparison
    fig, axs = plt.subplots(2, 2, figsize=(14, 12))
    bar_width = 0.35
    
    # Subplot 1: Frobenius Norm
    ax1 = axs[0, 0]
    index1 = np.arange(1)
    ax1.bar(index1, diffwave_metrics['frobenius_norm'], bar_width, label='DiffWave', color='blue', alpha=0.7)
    ax1.bar(index1 + bar_width, adaptive_metrics['frobenius_norm'], bar_width, label='Adaptive DiffWave', color='red', alpha=0.7)
    ax1.set_xlabel('Metric')
    ax1.set_ylabel('Value')
    ax1.set_title('Frobenius Norm Comparison')
    ax1.set_xticks(index1 + bar_width / 2)
    ax1.set_xticklabels(['Frobenius Norm'])
    ax1.legend()
    
    # Subplot 2: MSE and MAE
    ax2 = axs[0, 1]
    index2 = np.arange(2)
    mse_mae_diffwave = [diffwave_metrics['mse'], diffwave_metrics['mae']]
    mse_mae_adaptive = [adaptive_metrics['mse'], adaptive_metrics['mae']]
    ax2.bar(index2, mse_mae_diffwave, bar_width, label='DiffWave', color='blue', alpha=0.7)
    ax2.bar(index2 + bar_width, mse_mae_adaptive, bar_width, label='Adaptive DiffWave', color='red', alpha=0.7)
    ax2.set_xlabel('Metric')
    ax2.set_ylabel('Value')
    ax2.set_title('MSE and MAE Comparison')
    ax2.set_xticks(index2 + bar_width / 2)
    ax2.set_xticklabels(['MSE', 'MAE'])
    ax2.legend()
    
    # Subplot 3: Correlation
    ax3 = axs[1, 0]
    index3 = np.arange(1)
    ax3.bar(index3, diffwave_metrics['correlation'], bar_width, label='DiffWave', color='blue', alpha=0.7)
    ax3.bar(index3 + bar_width, adaptive_metrics['correlation'], bar_width, label='Adaptive DiffWave', color='red', alpha=0.7)
    ax3.set_xlabel('Metric')
    ax3.set_ylabel('Value')
    ax3.set_title('Correlation Coefficient Comparison')
    ax3.set_xticks(index3 + bar_width / 2)
    ax3.set_xticklabels(['Correlation'])
    ax3.legend()
    
    # Subplot 4: Wasserstein
    ax4 = axs[1, 1]
    index4 = np.arange(1)
    ax4.bar(index4, diffwave_metrics['wasserstein'], bar_width, label='DiffWave', color='blue', alpha=0.7)
    ax4.bar(index4 + bar_width, adaptive_metrics['wasserstein'], bar_width, label='Adaptive DiffWave', color='red', alpha=0.7)
    ax4.set_xlabel('Metric')
    ax4.set_ylabel('Value')
    ax4.set_title('Wasserstein Distance Comparison')
    ax4.set_xticks(index4 + bar_width / 2)
    ax4.set_xticklabels(['Wasserstein'])
    ax4.legend()
    plt.tight_layout()
    plt.savefig(f"{save_dir}/model_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Model performance comparison plot saved to: {save_dir}/model_comparison.png")
    for metric in metrics:
        diffwave_val = diffwave_metrics[metric]
        adaptive_val = adaptive_metrics[metric]
        
        if metric == 'correlation':  # Higher correlation is better
            is_better = adaptive_val > diffwave_val
            better_model = 'Adaptive DiffWave' if is_better else 'DiffWave'
            better_counts[better_model] += 1
        else:  # Lower is better for other metrics
            is_better = adaptive_val < diffwave_val
            better_model = 'Adaptive DiffWave' if is_better else 'DiffWave'
            better_counts[better_model] += 1
            
        print(f"{metric}: DiffWave={diffwave_val:.6f}, Adaptive DiffWave={adaptive_val:.6f}, Better model: {better_model}")
    
    print(f"\nSummary: DiffWave better metrics: {better_counts['DiffWave']}, Adaptive DiffWave better metrics: {better_counts['Adaptive DiffWave']}")
    winner = 'DiffWave' if better_counts['DiffWave'] > better_counts['Adaptive DiffWave'] else 'Adaptive DiffWave'
    print(f"Overall better performing model: {winner}")
    
    # Save results to text file
    with open(f"{save_dir}/evaluation_results.txt", "w") as f:
        f.write("====== Model Performance Evaluation Results ======\n\n")
        f.write("DiffWave Model Evaluation Results:\n")
        for metric, value in diffwave_metrics.items():
            f.write(f"{metric}: {value:.6f}\n")
        f.write("\nAdaptive DiffWave Model Evaluation Results:\n")
        for metric, value in adaptive_metrics.items():
            f.write(f"{metric}: {value:.6f}\n")
        f.write(f"\nSummary: DiffWave better metrics: {better_counts['DiffWave']}, Adaptive DiffWave better metrics: {better_counts['Adaptive DiffWave']}\n")
        f.write(f"Overall better performing model: {winner}\n")
    
    print(f"Evaluation results saved to: {save_dir}/evaluation_results.txt")

if __name__ == "__main__":
    main() 